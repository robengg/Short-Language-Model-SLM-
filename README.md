# ğŸ§  Small Language Model (SLM) â€“ CPU-Only Build from Scratch

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![License: CC](https://img.shields.io/badge/License-Creative%20Commons-lightgrey.svg)
![Status](https://img.shields.io/badge/Status-Experimental-orange.svg)

A **Small Language Model (SLM)** implementation designed to run **entirely on CPUs** with no GPU dependency.  
This project explores how lightweight language models can be built from scratch, trained on domain-specific datasets, and deployed for practical use cases such as retrieval-augmented generation (RAG), knowledge base search, and lightweight automation.

---

## ğŸŒ What is an SLM?

- **SLM (Small Language Model):**  
  A compact model trained with fewer parameters (typically millions to low billions) optimized for **efficiency, privacy, and CPU-only environments**.  
  Best for **edge devices, internal company servers, and specialized use cases**.

- **LLM (Large Language Model):**  
  Massive models (tens to hundreds of billions of parameters) trained on huge datasets.  
  Require **GPUs/TPUs, large-scale infrastructure, and higher cost** but achieve state-of-the-art generalization.

---

## âš–ï¸ LLM vs SLM â€“ Pros & Cons

| Feature            | LLM (Large)                                        | SLM (Small)                                        |
|--------------------|----------------------------------------------------|---------------------------------------------------|
| **Performance**    | High accuracy, broad generalization, multilingual  | Moderate accuracy, domain-specific is stronger     |
| **Hardware Needs** | Requires GPUs/TPUs, lots of VRAM, cluster setups   | Runs on CPUs, lightweight servers, or even laptops |
| **Cost**           | Expensive to train & host                          | Cheap to run, easy to maintain                     |
| **Use Cases**      | Chatbots, reasoning, coding assistants, open Q&A   | Internal bots, RAG with docs, automation, edge AI |
| **Privacy**        | Data often processed on external infra             | Can run fully on-prem / air-gapped                 |
| **Custom Training**| Expensive & resource-heavy                         | Easy to fine-tune on niche tasks                   |

---

## ğŸ› ï¸ Features

- âœ… CPU-only training and inference  
- âœ… Tokenizer balanced for **English + Japanese**  
- âœ… Retrieval-Augmented Generation (RAG) integration (FAISS backend)  
- âœ… Instruction-tuning dataset templates
- âœ… Educational build â€” from **tokenizer â†’ training â†’ inference pipeline**  

---

## ğŸ“‚ Project Structure

```
slm-cpu/
â”œâ”€â”€ data/             # Training datasets (plain text or JSONL)
â”‚   â”œâ”€â”€ en.txt        # Tiny English corpus
â”‚   â”œâ”€â”€ ja.txt        # Tiny Japanese corpus
â”‚   â””â”€â”€ instruct.jsonl # Example instruction-tuning dataset
â”œâ”€â”€ adapters/         # LoRA adapters saved here after training
â”œâ”€â”€ exports/          # Final merged/exported models
â”œâ”€â”€ scripts/          # Python scripts for training, generation, merging
â”‚   â”œâ”€â”€ train_lora_causal.py
â”‚   â”œâ”€â”€ train_sft_lora.py
â”‚   â”œâ”€â”€ generate.py
â”‚   â”œâ”€â”€ merge_lora.py
â”‚   â””â”€â”€ router.py
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start
LoRA stands for Low-Rank Adaptation of large language models.
Itâ€™s a method to fine-tune big models efficiently without retraining all their parameters.

# create venv
```
python3 -m venv .venv && source .venv/bin/activate
pip install -U pip wheel setuptools
pip install transformers datasets peft torch sentencepiece
```

# train LoRA (English)
```python
python scripts/train_lora_causal.py --model_path ~/models/distilgpt2 \
  --train_file data/en.txt --output_dir adapters/en_lora
```

# train LoRA (Japanese)
```python
python scripts/train_lora_causal.py \
  --model_path ~/models/rinna-jgpt2-small \
  --train_file data/ja.txt \
  --output_dir adapters/ja_lora
```
# generate
```python
python scripts/generate.py --base ~/models/distilgpt2 \
  --adapter adapters/en_lora --prompt "Write a haiku about teamwork"

```

## ğŸ” Example: RAG with Confluence Docs



---

## ğŸ“Š Roadmap

- [ ] Add multilingual tokenizer for EN/JA  
- [ ] Pre-train SLM on domain-specific text  
- [ ] Create Docker image for easy deployment  
- [ ] Release pretrained checkpoints  

---

## ğŸ“œ License

This project is licensed under the **Creative Commons Legal Code License** â€“ see the [LICENSE](LICENSE) file for details.  

---

## ğŸ¤ Contributing

Contributions, issues, and pull requests are welcome!  
If youâ€™d like to help, please open an issue first to discuss the scope.

---

## ğŸ™ Acknowledgments

- Inspired by the open-source ML community (Hugging Face, FAISS, SentenceTransformers)  
- Built for research, learning, and CPU-only environments  
- Developed with â¤ï¸ by [ROB ASHAD UR]
- Inspired by NVIDIA Research 
- https://research.nvidia.com/labs/lpr/slm-agents/

